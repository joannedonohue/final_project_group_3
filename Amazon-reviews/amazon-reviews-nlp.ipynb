{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# File paths\n",
    "file_path = '/content/drive/MyDrive/amazon_reviews_us_Watches_v1_00.tsv'\n",
    "sentiment_data_path = '/content/drive/MyDrive/amazon_sentiment_data.csv'\n",
    "\n",
    "# Load Data\n",
    "if os.path.exists(sentiment_data_path):\n",
    "    data = pd.read_csv(sentiment_data_path)\n",
    "else:\n",
    "    chunks = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip', quoting=3, chunksize=100000)\n",
    "    data_chunks = []\n",
    "\n",
    "    # Parallelized sentiment scoring\n",
    "    def process_chunk(chunk):\n",
    "        sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "        def sentiment_scores(sentence):\n",
    "            if not isinstance(sentence, str):\n",
    "                sentence = str(sentence)\n",
    "            return sid_obj.polarity_scores(sentence)['compound']\n",
    "\n",
    "        chunk['sentiment_score'] = chunk['review_body'].apply(sentiment_scores)\n",
    "        chunk['sentiment'] = chunk['sentiment_score'].apply(lambda x: 'positive' if x >= 0.05 else 'negative')\n",
    "        return chunk\n",
    "\n",
    "    # Process each chunk\n",
    "    data_chunks = Parallel(n_jobs=-1)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "    data = pd.concat(data_chunks, ignore_index=True)\n",
    "    data.to_csv(sentiment_data_path, index=False)\n",
    "\n",
    "# Aspect-Based Sentiment Analysis\n",
    "aspects = ['design', 'durability', 'price', 'brand']\n",
    "\n",
    "def extract_aspects(review):\n",
    "    if not isinstance(review, str):\n",
    "        review = str(review)\n",
    "    results = {}\n",
    "    for aspect in aspects:\n",
    "        if aspect in review:\n",
    "            polarity = TextBlob(review).sentiment.polarity\n",
    "            results[aspect] = 'positive' if polarity > 0 else 'negative'\n",
    "    return results\n",
    "\n",
    "data['aspect_sentiments'] = Parallel(n_jobs=-1)(delayed(extract_aspects)(review) for review in data['review_body'])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review_body'].fillna(''), data['sentiment'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Logistic Regression with GridSearch\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', class_weight='balanced'), param_grid, cv=3)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Optimized Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.3f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.3f}\")\n",
    "\n",
    "# LDA for Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42, max_iter=10)\n",
    "lda.fit(X_train_tfidf)\n",
    "\n",
    "print(\"\\nLDA Topics:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx}: \", [tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "\n",
    "# pyLDAvis Visualization\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.lda_model.prepare(lda, X_train_tfidf, tfidf, mds='tsne')  \n",
    "pyLDAvis.display(panel)\n",
    "\n",
    "# Sentiment Distribution Visualization\n",
    "sns.countplot(data['sentiment'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud for Negative Reviews\n",
    "negative_reviews = data[data['sentiment'] == 'negative']['review_body'].astype(str)\n",
    "wordcloud = WordCloud(width=800, height=400, stopwords='english').generate(\" \".join(negative_reviews))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Negative Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Aspect Sentiment Visualization\n",
    "aspect_data = pd.DataFrame(list(data['aspect_sentiments'].dropna().apply(pd.Series).stack().items()))\n",
    "aspect_data.columns = ['Index', 'Aspect Sentiment']\n",
    "aspect_counts = aspect_data['Aspect Sentiment'].value_counts()\n",
    "sns.barplot(x=aspect_counts.index, y=aspect_counts.values)\n",
    "plt.title('Aspect-Based Sentiment Distribution')\n",
    "plt.xlabel('Aspect Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
